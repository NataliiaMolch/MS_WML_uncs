import numpy as np
from joblib import Parallel, delayed
from functools import partial
import warnings


def intersection_over_union(mask1, mask2):
    """Compute IoU for 2 binary masks
    mask1 and mask2 should have same dimensions
    """
    return np.sum(mask1 * mask2) / np.sum(mask1 + mask2 - mask1 * mask2)

evaluated_uncertainty_measures = 'mean', 'logsum', 'iou_ap_det', 'mean_iou_det', 'mean_iou_det_true', 'rmi1', 'rmi2', 'mi1', 'mi2', 'mi1_true', 'mi2_true', 'mean confidence', 'logsum confidence', 'mean entropy_of_expected', 'logsum entropy_of_expected', 'mean expected_entropy', 'logsum expected_entropy', 'mean mutual_information', 'logsum mutual_information', 'mean epkl', 'logsum epkl', 'mean reverse_mutual_information', 'logsum reverse_mutual_information'


def single_lesion_uncertainty(cc_mask: np.ndarray, vox_unc_maps: dict, ens_pred_multi: np.ndarray,
                              ens_pred_multi_true: np.ndarray, ens_pred_prob: np.ndarray, epsilon: float = 1e-5):
    """ Compute different uncertainty measures for a single connected component, that include:
    * Mean from vox uncertainty (M Dojat)
    * Log-sum from vox uncertainty (T Alber)
    * Ensemble detection uncertainty (ours)
    * Andrey's proposal: RMI avg_prod and prod_avg, MI (avg_prod and prod_avg) x (same threshold and unique thresholds)
    on the structural scale
    :param cc_mask: binary lesion mask, shape [H, W, D]
    :param vox_unc_maps: a dictionary of voxel scale uncertainty maps for different voxel measures of uncertainty, shape [H, W, D]
    :param ens_pred_multi: labeled lesions masks generated by an ensemble of models - threshold tuned for ensemble, shape [M, H, W, D].
    :param ens_pred_prob: probabilities predicted by each model in ensemble
    :param ens_pred_multi_true: labeled lesions masks generated by an ensemble of models - thresholds tuned for individual models, shape [M, H, W, D].
    :param epsilon: small value to avoid -inf in logarithms
    :return: dictionary of lesion uncertainty measures
    :rtype: dict
    """
    def get_max_ious_ccmasks_ens(ens_multi):
        ens_ious_ = []
        ens_cc_masks_ = []
        for m in range(ens_multi.shape[0]):
            max_iou = 0.0
            max_cc_mask = np.zeros_like(cc_mask)
            for intersec_label in np.unique(cc_mask * ens_multi[m]):
                if intersec_label != 0.0:
                    lesion_m = (ens_multi[m] == intersec_label).astype(float)
                    iou = intersection_over_union(cc_mask, lesion_m)
                    if iou > max_iou:
                        max_iou = iou
                        max_cc_mask = lesion_m.copy()
            ens_ious_.append(max_iou)
            ens_cc_masks_.append(max_cc_mask)
        return ens_ious_, ens_cc_masks_

    ''' Check inputs '''
    if not cc_mask.shape == ens_pred_multi[0].shape == ens_pred_prob[0].shape:
        raise ValueError("Error in input dimensions:\n"
                         f"cc_mask: {cc_mask.shape}, "
                         f"ens_pred_prob: {ens_pred_prob.shape}, "
                         f"ens_pred_multi: {ens_pred_multi.shape}.")
    if not ens_pred_prob.shape == ens_pred_multi.shape:
        raise ValueError("Expecting same number of models in the ensemble:\n"
                         f"ens_pred_prob: {ens_pred_prob.shape}, "
                         f"ens_pred_multi: {ens_pred_multi.shape}.")
    res = dict()
    ''' Voxel-scale based uncertainties '''
    for unc_measure, vox_unc_map in vox_unc_maps.items():
        res.update({
            f"mean {unc_measure}": np.sum(vox_unc_map * cc_mask) / np.sum(cc_mask),
            f"logsum {unc_measure}": np.sum(np.log(vox_unc_map[cc_mask == 1]))
        })

    ''' Detection uncertainties '''
    # 1 - mean IoU of connected components on the ensemble's models prediction with cc_mask
    ens_ious, ens_cc_masks = get_max_ious_ccmasks_ens(ens_pred_multi)
    ens_ious_true, ens_cc_masks_true = get_max_ious_ccmasks_ens(ens_pred_multi_true)

    # 1 - intersection between the ensemble's models predictions and cc_mask aka lesion
    # don't compute mean IoU, but compute intersection and union between everything
    M = ens_pred_multi.shape[0]
    intersec = cc_mask.copy()
    union = cc_mask.copy()
    ens_pred_bin = (ens_pred_multi > 0.0).astype('float32')
    for m in range(M):
        intersec *= ens_pred_bin[m]
        union += ens_pred_bin[m]
    res.update({
        'iou_ap_det': 1 - np.sum(intersec) / np.sum(union),
        'mean_iou_det': 1 - np.mean(ens_ious), 'mean_iou_det_true': 1 - np.mean(ens_ious_true)
    })

    ''' RMI structural level '''
    L = cc_mask.sum()
    p_les = np.asarray([ens_pred_prob[m][cc_mask == 1] for m in range(M)])  # dim 0 - ens, dim 1 - vox
    denom = - np.log(p_les).mean()
    rmi1 = np.log(p_les.mean(axis=0)).mean() + denom
    rmi2 = np.log(p_les.prod(axis=1).mean() + epsilon) / L + denom
    res.update({'rmi1': rmi1, 'rmi2': rmi2})

    ''' MI structural level '''
    mi1, mi2 = 0, 0
    for m in range(M):
        cc_mask_m = ens_cc_masks[m].copy()
        Lm = cc_mask_m.sum()
        if not Lm == 0.0:
            numer = np.log(ens_pred_prob[m][cc_mask_m == 1]).mean()
            prob_les_m = np.asarray([ens_pred_prob[k][cc_mask_m == 1] for k in range(M)])  # dim 0 - ens, dim 1 - vox
            mi1 += numer - np.log(prob_les_m.mean(axis=0)).mean()
            mi2 += numer - np.log(prob_les_m.prod(axis=1).mean() + epsilon) / Lm
    res.update({'mi1': mi1, 'mi2': mi2})

    mi1, mi2 = 0, 0
    for m in range(M):
        cc_mask_m = ens_cc_masks_true[m].copy()
        Lm = cc_mask_m.sum()
        if not Lm == 0.0:
            numer = np.log(ens_pred_prob[m][cc_mask_m == 1]).mean()
            prob_les_m = np.asarray(
                [ens_pred_prob[k][cc_mask_m == 1] for k in range(M)])  # dim 0 - ens, dim 1 - vox
            mi1 += numer - np.log(prob_les_m.mean(axis=0)).mean()
            mi2 += numer - np.log(prob_les_m.prod(axis=1).mean() + epsilon) / Lm
    res.update({'mi1_true': mi1, 'mi2_true': mi2})
    return res


def lesions_uncertainty(y_pred_multi: np.ndarray, vox_unc_maps: dict, ens_pred_multi: np.ndarray, ens_pred_prob: np.ndarray,
                        ens_pred_multi_true: np.ndarray, n_jobs: int = None, dl: bool = True):
    """ Parallel evaluation of all lesion scale uncertainty measures for one subject.
    :param y_pred_multi: labeled lesion mask aka instance segmentation mask, shape [H, W, D]
    :param vox_unc_maps: dict with keys being names of voxel-scale uncertainty measures and values being corresponding 3D uncertainty maps of shape [H, W, D]
    :param ens_pred_multi: labeled lesions masks generated by an ensemble of models - threshold tuned for ensemble, shape [M, H, W, D].
    :param ens_pred_multi_true: labeled lesions masks generated by an ensemble of models - thresholds tuned for individual models, shape [M, H, W, D].
    :param ens_pred_prob: probability maps for all models in ensemble, shape [M, H, W, D]
    :param n_jobs: number of parallel workers
    :param dl: if True returns a dictionary of lists, where each key corresponds to a single lesion uncertainty measure,
    list contains uncertainty measures belonging to all lesions.
    :return:
    """
    def ld_to_dl(ld):
        keys = list(ld[0].keys())
        dl = dict(zip(keys, [[] for _ in keys]))
        for el in ld:
            for k in keys:
                v = el[k]
                dl[k].append(v)
        return dl

    warnings.filterwarnings("ignore")

    cc_labels = np.unique(y_pred_multi)
    cc_labels = cc_labels[cc_labels != 0.0]
    with Parallel(n_jobs=n_jobs) as parallel_backend:
        process = partial(single_lesion_uncertainty,
                          vox_unc_maps=vox_unc_maps,
                          ens_pred_prob=ens_pred_prob,
                          ens_pred_multi_true=ens_pred_multi_true,
                          ens_pred_multi=ens_pred_multi)
        les_uncs_list = parallel_backend(delayed(process)(
            cc_mask=(y_pred_multi == cc_label).astype("float")
        ) for cc_label in cc_labels)  # returns lists of dictionaries, but need dictionary of lists

    if dl:
        if not les_uncs_list:
            metrics = dict(zip(evaluated_uncertainty_measures, [[] for _ in evaluated_uncertainty_measures]))
            return metrics
        return ld_to_dl(les_uncs_list)
    else:
        return les_uncs_list


def lesions_uncertainty_maps(y_pred_multi, vox_uncs_map, ens_pred_multi, ens_pred_prob: np.ndarray,
                        ens_pred_multi_true: np.ndarray, n_jobs: int = None):
    """ Parallel evaluation of all lesion scale uncertainty measures for one subject returned as lesion uncertainty map for each evaluated measure..
    :param y_pred_multi: labeled lesion mask aka instance segmentation mask
    :param vox_uncs_map: voxel scale lesion uncertainty mask
    :param ens_pred_multi: labeled lesions masks generated by an ensemble of models - threshold tuned for ensemble, shape [M, H, W, D].
    :param ens_pred_multi_true: labeled lesions masks generated by an ensemble of models - thresholds tuned for individual models, shape [M, H, W, D].
    :param ens_pred_prob: probability maps for all models in ensemble, shape [M, H, W, D]
    :param n_jobs: number of parallel workers
    :return: dictionary with lesion uncertainty maps for each measure
    :rtype: dict
    """
    cc_labels = np.unique(y_pred_multi)
    cc_labels = cc_labels[cc_labels != 0.0]
    with Parallel(n_jobs=n_jobs) as parallel_backend:
        process = partial(single_lesion_uncertainty,
                          vox_uncs_map=vox_uncs_map,
                          ens_pred_prob=ens_pred_prob,
                          ens_pred_multi_true=ens_pred_multi_true,
                          ens_pred_multi=ens_pred_multi)
        les_uncs_list = parallel_backend(delayed(process)(
            cc_mask=(y_pred_multi == cc_label).astype("float")
        ) for cc_label in cc_labels)  # returns lists of dictionaries, but need dictionary of lists

    if les_uncs_list:
        les_uncs_measures = list(les_uncs_list[0].keys())
        results = dict(zip(les_uncs_measures, [np.zeros_like(y_pred_multi, dtype='float') for _ in les_uncs_measures]))
        # for each measure of uncertainty create a lesion uncertainty map
        for cc_label, uncs_dict in zip(cc_labels, les_uncs_list):
            for unc_name in les_uncs_measures:
                results[unc_name] += uncs_dict[unc_name] * (y_pred_multi == cc_label).astype('float')
        return results
